# ğŸ§© Projet Mini Store

## 1. Structure du projet
Voici la structure interne final du projet


```
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ app
â”‚Â Â  â”œâ”€â”€ app.py
â”‚Â Â  â”œâ”€â”€ requirements.txt
â”‚Â Â  â””â”€â”€ templates
â”‚Â Â      â””â”€â”€ index.html
â”œâ”€â”€ check-and-update.sh
â”œâ”€â”€ k8s
â”‚Â Â  â”œâ”€â”€ Deployment.yaml
â”‚Â Â  â”œâ”€â”€ Ingress.yaml
â”‚Â Â  â”œâ”€â”€ Service.yaml
â”‚Â Â  â”œâ”€â”€ auto-sync-configmap.yaml
â”‚Â Â  â”œâ”€â”€ auto-sync-cronjob.yaml
â”‚Â Â  â”œâ”€â”€ auto-sync-rbac.yaml
â”‚Â Â  â”œâ”€â”€ dashboard-admin.yaml
â”‚Â Â  â”œâ”€â”€ mongo
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cfg-pvc.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cfg-service.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cfg-statefulset.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongos-deployment.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongos-service.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongos.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard1-pvc.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard1-service.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard1-statefulset.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard2-pvc.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard2-service.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ shard2-statefulset.yaml
â”‚Â Â  â”œâ”€â”€ mongo-pvc.yaml
â”‚Â Â  â”œâ”€â”€ mongodb-replica-statefulset.yaml
â”‚Â Â  â”œâ”€â”€ mongodb-service.yaml
â”‚Â Â  â”œâ”€â”€ products.json
â”‚Â Â  â”œâ”€â”€ redis-service.yaml
â”‚Â Â  â”œâ”€â”€ redis-statefulset.yaml
â”‚Â Â  â”œâ”€â”€ test
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Deployment.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Ingress.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Service.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongo-pvc.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongodb-replica-statefulset.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ mongodb-service.yaml
â”‚Â Â  â””â”€â”€ watch-sync
â””â”€â”€ tests
    â””â”€â”€ test_app.py
```

### dossier App
Contient le code principal de l'application.

* `app.py`â€¯: le fichier principal de lâ€™application Flask Mini-Store. Contient les routes '/' pour lâ€™interface web et '/api/products' pour lâ€™API REST.

* `requirements.txt`â€¯: liste des dÃ©pendances Python nÃ©cessaires.

* `templates/index.html`â€¯: le template HTML principal pour lâ€™interface utilisateur.

### check-and-update.sh
Script shell permettant de synchroniser automatiquement le namespace test avec la derniÃ¨re image dÃ©ployÃ©e sur GitHub/DockerHub.

Il est utilisÃ© dans un CronJob Kubernetes pour vÃ©rifier toutes les 5 minutes si la version du namespace test est Ã  jour comparÃ© Ã  celle sur DockerHub, et dÃ©clencher un kubectl rollout restart si ce n'est pas le cas.

### dossier k8s
Contient tous les manifests Kubernetes pour dÃ©ployer lâ€™application et ses dÃ©pendances. Les manifests sont principalement pour le namespace dev, mais certains fichiers permettent aussi de crÃ©er un environnement de test ou de synchronisation automatique.

#### dossier mongo
Contient tous les manifests pour dÃ©ployer le cluster MongoDB sharded et ses services :

`cfg-pvc.yaml`â€¯: PVC pour les config servers MongoDB.

`cfg-service.yaml`â€¯: service pour exposer les config servers.

`cfg-statefulset.yaml`â€¯: StatefulSet pour les config servers.

`mongos-deployment.yaml`â€¯: dÃ©ploiement des routeurs mongos.

`mongos-service.yaml`â€¯: service exposant les mongos.

`mongos.yaml`â€¯: manifeste global pour le dÃ©ploiement complet des mongos.

`shard1-*.yaml / shard2-*.yaml`â€¯: StatefulSets, PVCs et services pour chaque shard du cluster MongoDB.

#### dossier test 

Manifests Kubernetes pour dÃ©ployer un environnement de test, isolÃ© du namespace dev :

* `Deployment.yaml`â€¯: dÃ©ploiement de lâ€™application Mini-Store pour tests.

* `Ingress.yaml`â€¯: expose lâ€™application de test via un Ingress.

* `Service.yaml`â€¯: service NodePort ou ClusterIP pour le test.

* `mongo-pvc.yaml / mongodb-replica-statefulset.yaml / mongodb-service.yaml`â€¯: MongoDB pour lâ€™environnement de test.

 #### autres fichiers dans k8s

`auto-sync-configmap.yaml`â€¯: ConfigMap pour configurer le script de sync automatique.

`auto-sync-cronjob.yaml`â€¯: CronJob qui exÃ©cute check-and-update.sh pÃ©riodiquement pour mettre Ã  jour le namespace test.

`auto-sync-rbac.yaml`â€¯: Role / RoleBinding pour donner les permissions nÃ©cessaires au CronJob.

`dashboard-admin.yaml`â€¯: manifeste pour dÃ©ployer un dashboard admin.

`mongo-pvc.yaml`â€¯: PVC pour MongoDB standalone ou replica set.

`mongodb-replica-statefulset.yaml`â€¯: StatefulSet pour MongoDB replica set.

`mongodb-service.yaml`â€¯: service exposant MongoDB.

`products.json`â€¯: fichier JSON contenant les produits initiaux.

`redis-service.yaml / redis-statefulset.yaml`â€¯: manifests pour dÃ©ployer Redis si utilisÃ©.

### dossier tests
Contient les tests unitaires de lâ€™application.

`test_app.py`â€¯: tests Python pour vÃ©rifier le comportement de app.py.

## 2. Namespaces

Lâ€™application principale est pour l'instant dÃ©ployÃ©e dans le namespace `dev`, Ã  chaque push sur github, la derniÃ¨re version de l'image `dev` est publiÃ© sur DockerHub et elle est alors en maximum 5 minutes pulled et deployÃ© par le Cronjob dans le namespace `test`

## 3. Cluster Setup Instructions

### 3.1 Local Setup

* comment installer et dÃ©marrer cluster
* dÃ©ploiement des namespaces dev et test 
* exposition via Ingress 

### 3.2 Cloud migration guide

* Gestion des secrets

## 4. CI/CD Pipeline 
GitHub Actions pipeline : description Ã©tape par Ã©tape.

Checkout, setup Python, install dependencies, run unit tests

Build Docker image, push DockerHub, check secrets

Comment le pipeline sâ€™intÃ¨gre avec le namespace test.

Option â€œauto-syncâ€ pour mettre Ã  jour le namespace test aprÃ¨s validation du pipeline.

## 5. Monitoring & Scaling Instructions 

### 5.1 Replica set
Pourquoi on utilise un replica set.
Manifests : StatefulSet, Service, PVC.
Comment accÃ©der Ã  la base pour debug/test (kubectl exec ... mongo).

### 5.2 Sharding
Pourquoi le sharding est utilisÃ©.

Architecture : config servers, shards, routeurs mongos.

Commandes pour vÃ©rifier lâ€™Ã©tat (sh.status()).

## 6. Monitoring & Scaling Instructions
dashboard

## 7. Guide pour les nouveaux developpeurs 

Installer Python et dÃ©pendances (requirements.txt).

Cloner le repo, setup cluster local.

Lancer les tests unitaires (pytest tests/).

Lancer lâ€™application en local (python app/app.py).

DÃ©ployer sur namespace dev/test (kubectl apply -f k8s/...).

OÃ¹ regarder logs et comment accÃ©der Ã  la base MongoDB.


# Ancien Readme 
## 3. DÃ©ploiement

Le dÃ©ploiement est rÃ©pliquÃ© avec deux pods.  

VÃ©rifier lâ€™Ã©tat des pods :

```bash
kubectl get pods -n dev
```


VÃ©rifier que les pods sont dans lâ€™Ã©tat READY 1/1 :
```
'kubectl get pods -n dev'
```
## 4. Services

Type de service : ClusterIP pour hello-k8s-service.

VÃ©rification : 
````
'kubectl get svc -n dev'
````

Le service est exposÃ© sur le port 80.

## 5. Ingress

Controller utilisÃ© : NGINX

Configuration dans le namespace dev
spec:
  ingressClassName: nginx

AccÃ¨s Ã  lâ€™application
```
curl http://localhost/
```

## 6. Docker

Lâ€™image de lâ€™application est construite Ã  partir du Dockerfile.
Le cluster peut utiliser soit une image locale, soit une image provenant dâ€™un dÃ©pÃ´t Docker.

## 7. Commandes principales utilisÃ©es
Appliquer les manifests :
```
kubectl apply -f k8s/Deployment.yaml -n dev
kubectl apply -f k8s/Service.yaml -n dev
kubectl apply -f k8s/Ingress.yaml -n dev
```

VÃ©rifications :
```
kubectl get pods -n dev
kubectl get svc -n dev
kubectl get ingress -n dev
```

Supprimer les ressources (en cas de conflit) :

```
kubectl delete ingress --all -n dev
kubectl delete deploy --all -n dev
kubectl delete svc --all -n dev
```

# CHATGPT 

kubectl prÃ©sent ?
kubectl version --client

Voir quel cluster kubernetes est actif
kubectl config current-context

vÃ©rifier s'il y a un ingress controller (nginx) installÃ©
kubectl get pods -A | grep ingress  true
kubectl get svc -A | grep ingress  true

VÃ©rifiez et crÃ©er les namespaces
kubectl get ns
kubectl create ns dev  true
kubectl create ns test  true
kubectl create ns prod || true
kubectl create ns kubernetes-dashboard 

Build L'image
docker build -t samirbch/mini-store:v13 .

Appliquer les manifests
kubectl apply -f k8s/ -R

ou 

kubectl apply -f k8s/mongo/ -n prod
kubectl apply -f k8s/ -n prod   fais attention: certains fichiers peuvent contenir namespace explicit

kubectl apply -f k8s/test/ -n test
kubectl apply -f k8s/ -n dev

VÃ©rifiez le post apply
kubectl get pods -n prod
kubectl get pods -n test
kubectl get pods -n dev

kubectl get svc -n prod
kubectl get svc -n test

kubectl get ingress -A

Ajouter
sudo nano /etc/hosts

'
127.0.0.1       dev.localhost
127.0.0.1       test.localhost'

Initialisez sharding
-Se connecter Ã  un pod config server :


kubectl exec -it -n dev mongo-cfg-0 -- mongosh --port 27019

-VÃ©rifier lâ€™Ã©tat du replica set :


rs.status()

-Initialiser le replica set :


rs.initiate({
  _id: "cfgReplSet",
  configsvr: true,
  members: [
    { _id: 0, host: "mongo-cfg-0.mongo-cfg-svc.dev.svc.cluster.local:27019" },
    { _id: 1, host: "mongo-cfg-1.mongo-cfg-svc.dev.svc.cluster.local:27019" },
    { _id: 2, host: "mongo-cfg-2.mongo-cfg-svc.dev.svc.cluster.local:27019" }
  ]
})

-RedÃ©marrer ton dÃ©ploiement Mongos pour quâ€™il se connecte aux config servers correctement :
kubectl rollout restart deployment mongos-deployment -n dev

# Samir
Guide Dashboard:A) Mise en place

Ã‰tape 1 : Appliquer le manifest officiel : kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.9.0/aio/deploy/recommended.yaml

Ã‰tape 2 : VÃ©rifier le dÃ©ploiement, les pods doivent Ãªtre en running:kubectl get pods -n kubernetes-dashboard

Etape 3 : CrÃ©er compte admin:CrÃ©er le ServiceAccount avec le rÃ´le admin dans un fichier Â«Â dashboard-admin.yamlÂ Â» (config complÃ¨te sur le git)Ensuite gÃ©nÃ©rer le token dâ€™accÃ¨s : kubectl -n kubernetes-dashboard create token admin-user


B) AccÃ¨s

Pour accÃ©der au Dashboard lancer : kubectl proxy
Aller dans navigateur : http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

Choisir lâ€™option Â«Â TokenÂ Â» et entrer le Token gÃ©nÃ©rer prÃ©cÃ©demment Une fois cela fait, le dashboard sera fonctionnel.

Guide Cache Redis:

Redis est utilisÃ© ici comme **systÃ¨me de cache** pour accÃ©lÃ©rer les requÃªtes en stockant temporairement les donnÃ©es.Il faut pour cela configurer un service pour accÃ©der aux pods et un statefulset pour gÃ©rer les pods avec un stockage persistant.

A) DÃ©ploiment 

Reprendre les configs Â«Â redis-service.yamlÂ Â» et Â«Â redis-statefulset.yamlÂ Â» sur GitHub et les apply avec :kubectl apply -f redis-service.yaml -n redis
kubectl apply -f redis-statefulset.yaml -n redis


VÃ©rifier que Redis est en cours dâ€™Ã©xecution: kubectl get pods -n redis -o wide
kubectl get pvc -n redis

B) Test

Pour tester si les pods redis marchent bien il suffit de sâ€™y connecter avec :
kubectl exec -it redis-0 -n redis -- redis-cli

Ensuite faire un : SET test "hello world"
Et ensuite un : GET test 
Pour obtenir la rÃ©ponse.

C) IntÃ©gration

Pour lâ€™intÃ©grer Ã  lâ€™app il faut que dans le Â«Â deployment.yamlÂ Â» il y ait une value "redis.redis.svc.cluster.local" qui correspond au Service headless crÃ©e dans les configs.Ensuite dans votre app il faut importer : import redis

Et le setup : REDIS_HOST = os.getenv("REDIS_HOST", "redis.dev.svc.cluster.local")  # nom du service Redis
REDIS_PORT = 6379
cache = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True)
4) VÃ©rification

Pour notre app, Ã  chaque produit ajouter, on peut vÃ©rifier le cache simplement avec :kubectl exec -it redis-1 -n dev -- redis-cli GET items