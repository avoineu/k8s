# ðŸ§© Projet Mini Store

## 1. Structure du projet
Voici la structure interne final du projet


```
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ app
â”‚Â Â  â”œâ”€â”€ app.py
â”‚Â Â  â”œâ”€â”€ requirements.txt
â”‚Â Â  â””â”€â”€ templates
â”‚Â Â      â””â”€â”€ index.html
â”œâ”€â”€ check-and-update.sh
â”œâ”€â”€ k8s
â”‚Â Â  â”œâ”€â”€ Deployment.yaml
â”‚Â Â  â”œâ”€â”€ Ingress.yaml
â”‚Â Â  â”œâ”€â”€ Service.yaml
â”‚Â Â  â”œâ”€â”€ auto-sync-configmap.yaml
â”‚Â Â  â”œâ”€â”€ auto-sync-cronjob.yaml
â”‚Â Â  â”œâ”€â”€ auto-sync-rbac.yaml
â”‚Â Â  â”œâ”€â”€ dashboard-admin.yaml
â”‚Â Â  â”œâ”€â”€ mongo
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cfg-pvc.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cfg-service.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cfg-statefulset.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongos-deployment.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongos-service.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongos.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard1-pvc.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard1-service.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard1-statefulset.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard2-pvc.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard2-service.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ shard2-statefulset.yaml
â”‚Â Â  â”œâ”€â”€ mongo-pvc.yaml
â”‚Â Â  â”œâ”€â”€ mongodb-replica-statefulset.yaml
â”‚Â Â  â”œâ”€â”€ mongodb-service.yaml
â”‚Â Â  â”œâ”€â”€ products.json
â”‚Â Â  â”œâ”€â”€ redis-service.yaml
â”‚Â Â  â”œâ”€â”€ redis-statefulset.yaml
â”‚Â Â  â”œâ”€â”€ test
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Deployment.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Ingress.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Service.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongo-pvc.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongodb-replica-statefulset.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ mongodb-service.yaml
â”‚Â Â  â””â”€â”€ watch-sync
â””â”€â”€ tests
    â””â”€â”€ test_app.py
```

### dossier App
Contient le code principal de l'application.

* `app.py`â€¯: le fichier principal de lâ€™application Flask Mini-Store. Contient les routes '/' pour lâ€™interface web et '/api/products' pour lâ€™API REST.

* `requirements.txt`â€¯: liste des dÃ©pendances Python nÃ©cessaires.

* `templates/index.html`â€¯: le template HTML principal pour lâ€™interface utilisateur.

### check-and-update.sh
Script shell permettant de synchroniser automatiquement le namespace test avec la derniÃ¨re image dÃ©ployÃ©e sur GitHub/DockerHub.

Il est utilisÃ© dans un CronJob Kubernetes pour vÃ©rifier toutes les 5 minutes si la version du namespace test est Ã  jour comparÃ© Ã  celle sur DockerHub, et dÃ©clencher un kubectl rollout restart si ce n'est pas le cas.

### dossier k8s
Contient tous les manifests Kubernetes pour dÃ©ployer lâ€™application et ses dÃ©pendances. Les manifests sont principalement pour le namespace dev, mais certains fichiers permettent aussi de crÃ©er un environnement de test ou de synchronisation automatique.

#### dossier mongo
Contient tous les manifests pour dÃ©ployer le cluster MongoDB sharded et ses services :

`cfg-pvc.yaml`â€¯: PVC pour les config servers MongoDB.

`cfg-service.yaml`â€¯: service pour exposer les config servers.

`cfg-statefulset.yaml`â€¯: StatefulSet pour les config servers.

`mongos-deployment.yaml`â€¯: dÃ©ploiement des routeurs mongos.

`mongos-service.yaml`â€¯: service exposant les mongos.

`mongos.yaml`â€¯: manifeste global pour le dÃ©ploiement complet des mongos.

`shard1-*.yaml / shard2-*.yaml`â€¯: StatefulSets, PVCs et services pour chaque shard du cluster MongoDB.

#### dossier test 

Manifests Kubernetes pour dÃ©ployer un environnement de test, isolÃ© du namespace dev :

* `Deployment.yaml`â€¯: dÃ©ploiement de lâ€™application Mini-Store pour tests.

* `Ingress.yaml`â€¯: expose lâ€™application de test via un Ingress.

* `Service.yaml`â€¯: service NodePort ou ClusterIP pour le test.

* `mongo-pvc.yaml / mongodb-replica-statefulset.yaml / mongodb-service.yaml`â€¯: MongoDB pour lâ€™environnement de test.

 #### autres fichiers dans k8s

`auto-sync-configmap.yaml`â€¯: ConfigMap pour configurer le script de sync automatique.

`auto-sync-cronjob.yaml`â€¯: CronJob qui exÃ©cute check-and-update.sh pÃ©riodiquement pour mettre Ã  jour le namespace test.

`auto-sync-rbac.yaml`â€¯: Role / RoleBinding pour donner les permissions nÃ©cessaires au CronJob.

`dashboard-admin.yaml`â€¯: manifeste pour dÃ©ployer un dashboard admin.

`mongo-pvc.yaml`â€¯: PVC pour MongoDB standalone ou replica set.

`mongodb-replica-statefulset.yaml`â€¯: StatefulSet pour MongoDB replica set.

`mongodb-service.yaml`â€¯: service exposant MongoDB.

`products.json`â€¯: fichier JSON contenant les produits initiaux.

`redis-service.yaml / redis-statefulset.yaml`â€¯: manifests pour dÃ©ployer Redis si utilisÃ©.

### dossier tests
Contient les tests unitaires de lâ€™application.

`test_app.py`â€¯: tests Python pour vÃ©rifier le comportement de app.py.

## 2. Namespaces

Lâ€™application principale est pour l'instant dÃ©ployÃ©e dans le namespace `dev`, Ã  chaque push sur github, la derniÃ¨re version de l'image `dev` est publiÃ© sur DockerHub et elle est alors en maximum 5 minutes pulled et deployÃ© par le Cronjob dans le namespace `test`

## 3. Cluster Setup Instructions

### 3.1 Local Setup

Le projet **Mini-Store** est conÃ§u pour fonctionner sur un cluster **Kubernetes** existant,  le dÃ©ploiement se fait directement sur le cluster configurÃ© via `kubectl`.

#### Ã‰tape 1 : VÃ©rification de lâ€™environnement Kubernetes

Avant de commencer, vÃ©rifier que `kubectl` est installÃ© et correctement connectÃ© Ã  votre cluster :

```bash
kubectl version --client
kubectl config current-context
kubectl get nodes
```

#### Ã‰tape 2 : CrÃ©ation des namespaces

Le projet utilise trois namespaces :

* **dev** : environnement principal de dÃ©veloppement
* **test** : environnement secondaire, oÃ¹ toute l'Ã©quipe de dÃ©veloppement peut tester la version final actuelle de l'application.
* **kubernetes-dashboard** : pour le monitoring du cluster

CrÃ©er les namespaces (si non existants) :
```
kubectl create ns dev
kubectl create ns test
kubectl create ns kubernetes-dashboard
```
#### Ã‰tape 3 : DÃ©ploiement de lâ€™application et des dÃ©pendances
Appliquer tous les manifests Kubernetes contenus dans le dossier `k8s/` :
```
kubectl apply -f k8s/ -R
```
Cela dÃ©ploie :
* Lâ€™application **Flask Mini-Store**
* **MongoDB** 
* **Redis** (cache)
* **CronJob** dâ€™auto-sync
* **Ingress Controller**

VÃ©rifier ensuite le statut des pods :

```
kubectl get pods -A
```

#### Ã‰tape 4 : Configuration du DNS local et accÃ¨s via Ingress
Pour accÃ©der Ã  lâ€™application via Ingress, ajouter les entrÃ©es suivantes dans `/etc/hosts` :
```
127.0.0.1       dev.localhost
127.0.0.1       test.localhost
```
Puis vÃ©rifier les Ingress actifs :
```
kubectl get ingress -A
```
Une fois le contrÃ´leur NGINX en place, lâ€™application est accessible sur :

* http://dev.localhost pour lâ€™environnement de dÃ©veloppement

* http://test.localhost pour lâ€™environnement de test

### 3.2 Cloud migration guide

#### Gestion des secrets 
Les credentials sensibles (comme `KUBE_CONFIG`, `DOCKER_USERNAME`, `DOCKER_PASSWORD`) sont stockÃ©s sous forme de **GitHub Secrets**.

## 4. CI/CD Pipeline 

Le dÃ©ploiement continu du projet est gÃ©rÃ© via **GitHub Actions**.  
Chaque push sur la branche principale dÃ©clenche automatiquement un pipeline complet qui assure la qualitÃ© du code, la crÃ©ation et la mise Ã  jour des images Docker, puis leur propagation dans le cluster.

### 4.1 Ã‰tapes du pipeline GitHub Actions

Le workflow suit la sÃ©quence suivante :

1. **Checkout du code**  
   TÃ©lÃ©chargement la derniÃ¨re version du dÃ©pÃ´t GitHub.

2. **Setup de Python**  
   Configuration Python et installation des dÃ©pendances.

3. **Installation des dÃ©pendances**

4. **Tests unitaires**  
   ExÃ©cution des tests contenus dans le dossier `tests/` pour valider lâ€™application.

5. **Build de lâ€™image Docker**

6. **Connexion Ã  DockerHub**

7. **Push de lâ€™image Docker**

8. **VÃ©rification des secrets**  
   Le pipeline vÃ©rifie que les secrets `KUBE_CONFIG`, `DOCKER_USERNAME`, `DOCKER_PASSWORD` sont bien prÃ©sents avant dâ€™exÃ©cuter les Ã©tapes critiques.

### 4.2 IntÃ©gration avec Kubernetes (namespace test)

Une fois lâ€™image publiÃ©e sur DockerHub, la mise Ã  jour du cluster est automatisÃ©e par un **CronJob Kubernetes** via le script `check-and-update.sh`.

### Fonctionnement

* Le CronJob sâ€™exÃ©cute toutes les 5 minutes.
* Il vÃ©rifie si la derniÃ¨re image DockerHub correspond Ã  celle actuellement utilisÃ©e par le namespace **test**.
* Si une nouvelle image est disponible, le script dÃ©clenche le dÃ©ploiement.

Cela garantit que le namespace **test** reflÃ¨te toujours la version la plus rÃ©cente validÃ©e par le pipeline.

### Avantage

Ce mÃ©canisme permet de dÃ©coupler complÃ¨tement :  

* le **build et le push** (via GitHub Actions)  
* le **dÃ©ploiement** (via Kubernetes + CronJob)  

assurant ainsi une intÃ©gration continue et un dÃ©ploiement continu (**CI/CD**) entiÃ¨rement automatisÃ©.

## 5. Monitoring, Scaling Instructions & Redis

### 5.1 Replica set

#### Concept

Il sâ€™agit dâ€™un ensemble de pods MongoDB contenant la mÃªme base de donnÃ©es :

* Un Primary : gÃ¨re les Ã©critures et rÃ©pliques vers les secondaires.

* Un ou plusieurs Secondaries : rÃ©pliquent les donnÃ©es et peuvent prendre le relais en cas de panne du Primary.

#### Setup 

Trois fichiers sont nÃ©cessaires pour dÃ©ployer le Replica Set :

* `mongodb-statefulset.yaml` :
DÃ©finit les 3 pods MongoDB avec `--replSet rs0` pour activer la rÃ©plication.
Chaque pod dispose dâ€™un stockage persistant via un volumeClaimTemplate.

* `mongodb-service.yaml` :
Service headless (clusterIP: None) utilisÃ© pour la dÃ©couverte automatique des pods (mongodb-0, mongodb-1, mongodb-2).

* `mongo-pvc.yaml` :
DÃ©finit les PersistentVolumeClaims (PVC) qui assurent la conservation des donnÃ©es mÃªme si un pod redÃ©marre.

Une fois cela fait, il suffit d'apply les fichiers :
```
`kubectl apply -f mongo-pvc.yaml -n test`

`kubectl apply -f mongodb-service.yaml -n test`

`kubectl apply -f mongodb-statefulset.yaml -n test`
```

Pour se connecter au premier pod :
```
'kubectl exec -it mongodb-0 -n test -- mongosh'
```

Ensuite il faut initialiser le Replica Set :

```
rs.initiate({
  _id: "rs0",

  members: [
    { _id: 0, host: "mongodb-0.mongodb-service.test.svc.cluster.local:27017" },

    { _id: 1, host: "mongodb-1.mongodb-service.test.svc.cluster.local:27017" },

    { _id: 2, host: "mongodb-2.mongodb-service.test.svc.cluster.local:27017" }
  ]
})
```
Pour vÃ©rifier :
```
rs.status()
```
#### AccÃ¨s et debug
Pour accÃ©der au pod et ajouter un item dans la DB :
```
`kubectl exec -it mongodb-0 -n test -- mongosh `

`use shop `

`db.items.insertOne({ id: 1, name: "T-shirt", price: 19.99 })`

`db.items.find() `
```
### 5.2 Dashboard
#### A) Mise en place

* Ã‰tape 1 : Appliquer le manifest officiel : 
```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.9.0/aio/deploy/recommended.yaml`
```

* Ã‰tape 2 : VÃ©rifier le dÃ©ploiement, les pods doivent Ãªtre en running: 
```
kubectl get pods -n kubernetes-dashboard
```

* Etape 3 : CrÃ©er compte admin, il suffit de crÃ©er le ServiceAccount avec le rÃ´le admin dans un fichier `dashboard-admin.yaml` (config complÃ¨te sur le git). Ensuite gÃ©nÃ©rer le token dâ€™accÃ¨s : 
```
kubectl -n kubernetes-dashboard create token admin-user
```

#### B) AccÃ¨s

Pour accÃ©der au Dashboard lancer : 
```
kubectl proxy
```

Aller sur votre navigateur et lancer : http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

Choisir lâ€™option Â« Token Â» et entrer le Token gÃ©nÃ©rer prÃ©cÃ©demment.
Une fois cela fait, le dashboard sera fonctionnel.


### 5.3 Sharding

Le Sharding est une technique de scaling horizontal utilisÃ©e pour rÃ©partir les donnÃ©es dâ€™une base MongoDB sur plusieurs nÅ“uds.

#### Architecture

`config-server-statefulset.yaml` â†’ dÃ©ploie les Config Servers qui stockent les mÃ©tadonnÃ©es du cluster.

`shard1-statefulset.yaml, shard2-statefulset.yaml` â†’ dÃ©ploient les Shards contenant les donnÃ©es rÃ©elles (chaque shard est un ReplicaSet).

`mongos-deployment.yaml` â†’ dÃ©ploie le Router (mongos) qui oriente les requÃªtes vers le bon shard.



#### Initialisation 

Se connecter au pod : 'kubectl exec -it mongos-xxxxxxxx -n test -- mongosh'


Puis executer les commandes suivantes afin d'initialiser le config-server, les shards et ensuite d'activer le sharding :

```
// Init config server
rs.initiate({
  _id: "rs-config-server",
  configsvr: true,
  members: [
    { _id: 0, host: "config-server-0.config-server.test.svc.cluster.local:27019" },
    { _id: 1, host: "config-server-1.config-server.test.svc.cluster.local:27019" },
    { _id: 2, host: "config-server-2.config-server.test.svc.cluster.local:27019" }
  ]
})

// Init shard1
rs.initiate({
  _id: "rs-shard1",
  members: [
    { _id: 0, host: "shard1-0.shard1.test.svc.cluster.local:27018" },
    { _id: 1, host: "shard1-1.shard1.test.svc.cluster.local:27018" },
    { _id: 2, host: "shard1-2.shard1.test.svc.cluster.local:27018" }
  ]
})

// Init shard2
rs.initiate({
  _id: "rs-shard2",
  members: [
    { _id: 0, host: "shard2-0.shard2.test.svc.cluster.local:27018" },
    { _id: 1, host: "shard2-1.shard2.test.svc.cluster.local:27018" },
    { _id: 2, host: "shard2-2.shard2.test.svc.cluster.local:27018" }
  ]
})

// Depuis mongos : ajouter les shards
sh.addShard("rs-shard1/shard1-0.shard1.test.svc.cluster.local:27018")
sh.addShard("rs-shard2/shard2-0.shard2.test.svc.cluster.local:27018")

// Activer le sharding sur la base
sh.enableSharding("shop")

// DÃ©finir la clÃ© de sharding
db.items.createIndex({ id: "hashed" })
sh.shardCollection("shop.items", { id: "hashed" })
```


### 5.4 Redis

Redis est utilisÃ© ici comme systÃ¨me de cache pour accÃ©lÃ©rer les requÃªtes en stockant temporairement les donnÃ©es. Il faut pour cela configurer un service pour accÃ©der aux pods et un statefulset pour gÃ©rer les pods avec un stockage persistant.

#### DÃ©ploiment

Reprendre les configs ` redis-service.yaml `et `redis-statefulset.yaml ` sur GitHub et les apply avec : 
```
kubectl apply -f redis-service.yaml -n redis

kubectl apply -f redis-statefulset.yaml -n redis
```


VÃ©rifier que Redis est en cours dâ€™Ã©xecution: 

```
kubectl get pods -n redis -o wide

kubectl get pvc -n redis
```


#### Test

Pour tester si les pods redis marchent bien il suffit de sâ€™y connecter avec : 

```
kubectl exec -it redis-0 -n redis -- redis-cli
```


Ensuite faireces deux commandes pour obtenir la rÃ©ponse. : 

```
SET test "hello world"

GET test
```


#### IntÃ©gration

Pour lâ€™intÃ©grer Ã  lâ€™app il faut que dans le `deployment.yaml` il y ait une value **redis.redis.svc.cluster.local** qui correspond au Service headless crÃ©e dans les configs. Ensuite dans votre app il faut importer avec : 
```
import redis
```


Et le setup : 
```
REDIS_HOST = os.getenv("REDIS_HOST", "redis.dev.svc.cluster.local") # nom du service Redis REDIS_PORT = 6379 cache = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True) 
```



#### VÃ©rification

Pour notre app, Ã  chaque produit ajouter, on peut vÃ©rifier le cache simplement avec :
```
kubectl exec -it redis-1 -n dev -- redis-cli GET items
```



## 6. Guide pour les nouveaux developpeurs 

### 6.1 Installation et prÃ©paration

Installer Python 3.10+

Installer les dÃ©pendances : `pip install -r requirements.txt`

Cloner le projet : 
`git clone https://github.com/avoineu/k8s.git`

`cd <nom-du-projet>`


### 6.2 Lancer le cluster local

Appliquer les manifests Kubernetes : `kubectl apply -f k8s/`

VÃ©rifier les pods en cours d'Ã©xecution : `kubectl get pods -n dev`

### 6.3 Tests unitaires

Pour effectuer dles tests unitaires :

-Soit lancer `pytest test_app.py/`

-Soit push sur GitHub

### 6.4 Lancer lâ€™application en local

Dans Kubernetes â†’ kubectl apply suffit pour que les pods sâ€™exÃ©cutent automatiquement.

Local (hors K8s) â†’ on peut lancer python app/app.py pour tester sans cluster.

### 6.5 DÃ©ploiement sur cluster

Pour dÃ©ployer sur un namespace : `kubectl apply -f k8s/ -n dev`

### 7.6 Logs & Debug

Voir les logs : `kubectl logs -f <nom-du-pod> -n dev`

AccÃ¨s Ã  MongoDB pour inspection : `kubectl exec -it mongodb-0 -n dev -- mongosh `

`use shop `

`db.items.find().pretty()`