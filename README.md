# ðŸ§© Projet Mini Store

## 1. Structure du projet
Voici la structure interne final du projet


```
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ app
â”‚Â Â  â”œâ”€â”€ app.py
â”‚Â Â  â”œâ”€â”€ requirements.txt
â”‚Â Â  â””â”€â”€ templates
â”‚Â Â      â””â”€â”€ index.html
â”œâ”€â”€ check-and-update.sh
â”œâ”€â”€ k8s
â”‚Â Â  â”œâ”€â”€ Deployment.yaml
â”‚Â Â  â”œâ”€â”€ Ingress.yaml
â”‚Â Â  â”œâ”€â”€ Service.yaml
â”‚Â Â  â”œâ”€â”€ auto-sync-configmap.yaml
â”‚Â Â  â”œâ”€â”€ auto-sync-cronjob.yaml
â”‚Â Â  â”œâ”€â”€ auto-sync-rbac.yaml
â”‚Â Â  â”œâ”€â”€ dashboard-admin.yaml
â”‚Â Â  â”œâ”€â”€ mongo
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cfg-pvc.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cfg-service.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ cfg-statefulset.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongos-deployment.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongos-service.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongos.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard1-pvc.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard1-service.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard1-statefulset.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard2-pvc.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ shard2-service.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ shard2-statefulset.yaml
â”‚Â Â  â”œâ”€â”€ mongo-pvc.yaml
â”‚Â Â  â”œâ”€â”€ mongodb-replica-statefulset.yaml
â”‚Â Â  â”œâ”€â”€ mongodb-service.yaml
â”‚Â Â  â”œâ”€â”€ prod
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Deployment.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Ingress.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Service.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongodb-replica-statefulset.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ mongodb-service.yaml
â”‚Â Â  â”œâ”€â”€ products.json
â”‚Â Â  â”œâ”€â”€ redis-service.yaml
â”‚Â Â  â”œâ”€â”€ redis-statefulset.yaml
â”‚Â Â  â”œâ”€â”€ test
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Deployment.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Ingress.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ Service.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongo-pvc.yaml
â”‚Â Â  â”‚Â Â  â”œâ”€â”€ mongodb-replica-statefulset.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ mongodb-service.yaml
â”‚Â Â  â””â”€â”€ watch-sync
â””â”€â”€ tests
    â””â”€â”€ test_app.py
```

### dossier App
Contient le code principal de l'application.

* `app.py`â€¯: le fichier principal de lâ€™application Flask Mini-Store. Contient les routes '/' pour lâ€™interface web et '/api/products' pour lâ€™API REST.

* `requirements.txt`â€¯: liste des dÃ©pendances Python nÃ©cessaires.

* `templates/index.html`â€¯: le template HTML principal pour lâ€™interface utilisateur.

### check-and-update.sh
Script shell permettant de synchroniser automatiquement le namespace test avec la derniÃ¨re image dÃ©ployÃ©e sur GitHub/DockerHub.

Il est utilisÃ© dans un CronJob Kubernetes pour vÃ©rifier toutes les 5 minutes si la version du namespace test est Ã  jour comparÃ© Ã  celle sur DockerHub, et dÃ©clencher un kubectl rollout restart si ce n'est pas le cas.

### dossier k8s
Contient tous les manifests Kubernetes pour dÃ©ployer lâ€™application et ses dÃ©pendances. Les manifests sont principalement pour le namespace dev, mais certains fichiers permettent aussi de crÃ©er un environnement de test, production ou de synchronisation automatique.

#### dossier mongo
Contient tous les manifests pour dÃ©ployer le cluster MongoDB sharded et ses services :

`cfg-pvc.yaml`â€¯: PVC pour les config servers MongoDB.

`cfg-service.yaml`â€¯: service pour exposer les config servers.

`cfg-statefulset.yaml`â€¯: StatefulSet pour les config servers.

`mongos-deployment.yaml`â€¯: dÃ©ploiement des routeurs mongos.

`mongos-service.yaml`â€¯: service exposant les mongos.

`mongos.yaml`â€¯: manifeste global pour le dÃ©ploiement complet des mongos.

`shard1-*.yaml / shard2-*.yaml`â€¯: StatefulSets, PVCs et services pour chaque shard du cluster MongoDB.

#### dossier test 

Manifests Kubernetes pour dÃ©ployer un environnement de test, isolÃ© du namespace dev :

* `Deployment.yaml`â€¯: dÃ©ploiement de lâ€™application Mini-Store pour test.

* `Ingress.yaml`â€¯: expose lâ€™application de test via un Ingress.

* `Service.yaml`â€¯: service NodePort ou ClusterIP pour le test.

* `mongo-pvc.yaml / mongodb-replica-statefulset.yaml / mongodb-service.yaml`â€¯: MongoDB pour lâ€™environnement de test.

#### dossier prod
Manifests Kubernetes pour dÃ©ployer un environnement de production, isolÃ© du namespace dev et test :
* `Deployment.yaml`â€¯: dÃ©ploiement de lâ€™application Mini-Store pour prod.

* `Ingress.yaml`â€¯: expose lâ€™application de prod via un Ingress.

* `Service.yaml`â€¯: service NodePort ou ClusterIP pour la prod.

* `mongodb-replica-statefulset.yaml / mongodb-service.yaml`â€¯: MongoDB pour lâ€™environnement de prod.

 #### autres fichiers dans k8s

`auto-sync-configmap.yaml`â€¯: ConfigMap pour configurer le script de sync automatique.

`auto-sync-cronjob.yaml`â€¯: CronJob qui exÃ©cute check-and-update.sh pÃ©riodiquement pour mettre Ã  jour le namespace test.

`auto-sync-rbac.yaml`â€¯: Role / RoleBinding pour donner les permissions nÃ©cessaires au CronJob.

`dashboard-admin.yaml`â€¯: manifeste pour dÃ©ployer un dashboard admin.

`mongo-pvc.yaml`â€¯: PVC pour MongoDB standalone ou replica set.

`mongodb-replica-statefulset.yaml`â€¯: StatefulSet pour MongoDB replica set.

`mongodb-service.yaml`â€¯: service exposant MongoDB.

`products.json`â€¯: fichier JSON contenant les produits initiaux.

`redis-service.yaml / redis-statefulset.yaml`â€¯: manifests pour dÃ©ployer Redis.

### dossier tests
Contient les tests unitaires de lâ€™application.

`test_app.py`â€¯: tests Python pour vÃ©rifier le comportement de app.py.

## 2. Namespaces

Lâ€™application principale est pour l'instant dÃ©ployÃ©e dans le namespace `dev`, Ã  chaque push sur github, la derniÃ¨re version de l'image `dev` est publiÃ© sur DockerHub et elle est alors en maximum 5 minutes pulled et deployÃ© par le Cronjob dans le namespace `test`. Le namespace `test` permet de tester une version plus au point de l'application. Le namespace `prod` met Ã  disposition l'application auprÃ¨s de tout les utilisateurs. 

## 3. Cluster Setup Instructions

### 3.1 Local Setup

Le projet **Mini-Store** est conÃ§u pour fonctionner sur un cluster **Kubernetes** existant,  le dÃ©ploiement se fait directement sur le cluster configurÃ© via `kubectl`.

#### Ã‰tape 1 : VÃ©rification de lâ€™environnement Kubernetes

Avant de commencer, vÃ©rifier que `kubectl` est installÃ© et correctement connectÃ© Ã  votre cluster :

```bash
kubectl version --client
kubectl config current-context
kubectl get nodes
```

#### Ã‰tape 2 : CrÃ©ation des namespaces

Le projet utilise quatre namespaces :

* **dev** : environnement principal de dÃ©veloppement
* **test** : environnement secondaire, oÃ¹ toute l'Ã©quipe de dÃ©veloppement peut tester la version final actuelle de l'application.
* **prod** : environnement de production, hÃ©bergeant la version stable et validÃ©e de lâ€™application, accessible aux utilisateurs finaux  
* **kubernetes-dashboard** : pour le monitoring du cluster

CrÃ©er les namespaces (si non existants) :
```
kubectl create ns dev
kubectl create ns test
kubectl create ns prod
kubectl create ns kubernetes-dashboard
```
#### Ã‰tape 3 : DÃ©ploiement de lâ€™application et des dÃ©pendances
Appliquer tous les manifests Kubernetes contenus dans le dossier `k8s/` :
```
kubectl apply -f k8s/ -R
```
Cela dÃ©ploie :
* Lâ€™application **Flask Mini-Store**
* **MongoDB** (base de donnÃ©es principale)
* **Redis** (cache)
* **CronJob** dâ€™auto-sync
* **Ingress Controller**
* les **Services, ConfigMaps, et secrets** nÃ©cessaires

VÃ©rifier ensuite le statut des pods et des ressources dÃ©ployÃ©es : :

```
kubectl get pods -A
kubectl get svc -A
kubectl get pvc -A
```
Tous les pods doivent Ãªtre dans lâ€™Ã©tat Running ou Completed avant de passer Ã  lâ€™Ã©tape suivante.

#### Ã‰tape 4 : Configuration du DNS local et accÃ¨s via Ingress
Pour accÃ©der Ã  lâ€™application via Ingress, ajouter les entrÃ©es suivantes dans le fichier `/etc/hosts` :
```
127.0.0.1       dev.localhost
127.0.0.1       test.localhost
127.0.0.1       prod.localhost
```
Puis vÃ©rifier les Ingress actifs :
```
kubectl get ingress -A
```
Une fois le contrÃ´leur NGINX dÃ©ployÃ© et configurÃ©, les environnements deviennent accessibles :

* http://dev.localhost pour lâ€™environnement de dÃ©veloppement

* http://test.localhost pour lâ€™environnement de test

* http://prod.localhost pour lâ€™environnement de production

#### Ã‰tape 5 : VÃ©rification du cluster
S'assurer que tout le cluster fonctionne correctement : 
```
kubectl get all -A
```

## 4. CI/CD Pipeline 

Le dÃ©ploiement continu du projet est gÃ©rÃ© via **GitHub Actions**.  
Chaque push sur la branche principale dÃ©clenche automatiquement un pipeline complet qui assure la qualitÃ© du code, la crÃ©ation et la mise Ã  jour des images Docker, puis leur propagation dans le cluster.

#### Gestion des secrets 
Les credentials sensibles (comme `KUBE_CONFIG`, `DOCKER_USERNAME`, `DOCKER_PASSWORD`) sont stockÃ©s sous forme de **GitHub Secrets** afin de sÃ©curiser les accÃ¨s lors du dÃ©ploiement automatique.

* **`KUBE_CONFIG`** :  Contient le contenu du fichier `~/.kube/config` permettant Ã  GitHub Actions dâ€™interagir avec le cluster Kubernetes. Pour lâ€™obtenir, exÃ©cuter sur la machine ou le cluster concernÃ© : 
```
  cat ~/.kube/config
  ```

* **`DOCKER_USERNAME`** : Nom dâ€™utilisateur du compte Docker Hub utilisÃ© pour builder et pousser les images Docker.

* **`DOCKER_PASSWORD`** : Mot de passe ou token dâ€™accÃ¨s personnel associÃ© au compte Docker Hub (Settings â†’ Security â†’ New Access Token).

### 4.1 Ã‰tapes du pipeline GitHub Actions

Le workflow suit la sÃ©quence suivante :

1. **Checkout du code**  
   TÃ©lÃ©chargement la derniÃ¨re version du dÃ©pÃ´t GitHub.

2. **Setup de Python**  
   Configuration Python et installation des dÃ©pendances.

3. **Installation des dÃ©pendances**

4. **Tests unitaires**  
   ExÃ©cution des tests contenus dans le dossier `tests/` pour valider lâ€™application.

5. **Build de lâ€™image Docker**

6. **Connexion Ã  DockerHub**

7. **Push de lâ€™image Docker**

8. **VÃ©rification des secrets**  
   Le pipeline vÃ©rifie que les secrets `KUBE_CONFIG`, `DOCKER_USERNAME`, `DOCKER_PASSWORD` sont bien prÃ©sents avant dâ€™exÃ©cuter les Ã©tapes critiques.

### 4.2 IntÃ©gration avec Kubernetes (namespace test)

Une fois lâ€™image publiÃ©e sur DockerHub, la mise Ã  jour du cluster est automatisÃ©e par un **CronJob Kubernetes** via le script `check-and-update.sh`.

### Fonctionnement

* Le CronJob sâ€™exÃ©cute toutes les 5 minutes.
* Il vÃ©rifie si la derniÃ¨re image DockerHub correspond Ã  celle actuellement utilisÃ©e par le namespace **test**.
* Si une nouvelle image est disponible, le script dÃ©clenche le dÃ©ploiement.

Cela garantit que le namespace **test** reflÃ¨te toujours la version la plus rÃ©cente validÃ©e par le pipeline.

### Avantages

Ce mÃ©canisme permet de dÃ©coupler complÃ¨tement :  

* le **build et le push** (via GitHub Actions)  
* le **dÃ©ploiement** (via Kubernetes + CronJob)  

assurant ainsi une intÃ©gration continue et un dÃ©ploiement continu (**CI/CD**) entiÃ¨rement automatisÃ©.

## 5. Monitoring, Scaling Instructions & Redis

### 5.1 Replica set

#### Concept

Il sâ€™agit dâ€™un ensemble de pods MongoDB contenant la mÃªme base de donnÃ©es :

* Un Primary : gÃ¨re les Ã©critures et rÃ©pliques vers les secondaires.

* Un ou plusieurs Secondaries : rÃ©pliquent les donnÃ©es et peuvent prendre le relais en cas de panne du Primary.

#### Setup 

Trois fichiers sont nÃ©cessaires pour dÃ©ployer le Replica Set :

* `mongodb-statefulset.yaml` :
DÃ©finit les 3 pods MongoDB avec `--replSet rs0` pour activer la rÃ©plication.
Chaque pod dispose dâ€™un stockage persistant via un volumeClaimTemplate.

* `mongodb-service.yaml` :
Service headless (clusterIP: None) utilisÃ© pour la dÃ©couverte automatique des pods (mongodb-0, mongodb-1, mongodb-2).

* `mongo-pvc.yaml` :
DÃ©finit les PersistentVolumeClaims (PVC) qui assurent la conservation des donnÃ©es mÃªme si un pod redÃ©marre.

Une fois cela fait, il suffit d'apply les fichiers Ã  l'aide de ces commandes :
```
`kubectl apply -f mongo-pvc.yaml -n test`

`kubectl apply -f mongodb-service.yaml -n test`

`kubectl apply -f mongodb-statefulset.yaml -n test`
```

Il faut alors se connecter au primary pod (genÃ©ralement le premier pod) Ã  l'aide de cette commande :
```
'kubectl exec -it mongodb-0 -n test -- mongosh'
```

Alors, il faut initialiser le Replica Set :

```
rs.initiate({
  _id: "rs0",

  members: [
    { _id: 0, host: "mongodb-0.mongodb-service.test.svc.cluster.local:27017" },

    { _id: 1, host: "mongodb-1.mongodb-service.test.svc.cluster.local:27017" },

    { _id: 2, host: "mongodb-2.mongodb-service.test.svc.cluster.local:27017" }
  ]
})
```
Pour vÃ©rifier :
```
rs.status()
```
#### AccÃ¨s et debug
Pour accÃ©der au pod, ajouter un item et afficher le contenu de la DB :
```
kubectl exec -it mongodb-0 -n test -- mongosh 

use shop 

db.items.insertOne({ id: 1, name: "T-shirt", price: 19.99 })

db.items.find() 
```
### 5.2 Dashboard
#### A) Mise en place

* Ã‰tape 1 : Appliquer le manifest officiel : 
```
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.9.0/aio/deploy/recommended.yaml`
```

* Ã‰tape 2 : VÃ©rifier le dÃ©ploiement, les pods doivent Ãªtre en Ã©tat **running** : 
```
kubectl get pods -n kubernetes-dashboard
```

* Etape 3 : CrÃ©er compte admin, il suffit de crÃ©er le ServiceAccount avec le rÃ´le admin dans un fichier `dashboard-admin.yaml` (config complÃ¨te sur le git). Ensuite gÃ©nÃ©rer le token dâ€™accÃ¨s : 
```
kubectl -n kubernetes-dashboard create token admin-user
```

#### B) AccÃ¨s

Pour accÃ©der au Dashboard lancer : 
```
kubectl proxy
```

Aller sur votre navigateur et lancer : http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/

Choisir lâ€™option Â« Token Â» et entrer le Token gÃ©nÃ©rÃ© prÃ©cÃ©demment.
Une fois cela fait, le dashboard sera fonctionnel. 

Il suffit alors d'aller dans la section `deployments`pour mettre Ã  l'Ã©chelle (scaling) les pods qu'on veut au nombre que l'on veut. On peut Ã©galement depuis le dashboard avoir une idÃ©e en instantanÃ©e de l'usage du CPU et de la mÃ©moire faites par nos pods.


### 5.3 Sharding

Le Sharding est une technique de scaling horizontal utilisÃ©e pour rÃ©partir les donnÃ©es dâ€™une base MongoDB sur plusieurs nÅ“uds.

#### Architecture
Lâ€™infrastructure MongoDB en mode Sharded Cluster repose sur **trois composants principaux** :

1. **Config Servers**  
   - Stockent les **mÃ©tadonnÃ©es du cluster**, notamment la rÃ©partition des chunks (fragments de donnÃ©es) entre les shards.  
   - DÃ©ployÃ©s via le fichier :  
     ```yaml
     config-server-statefulset.yaml
     ```
   - Ces pods sont regroupÃ©s dans un **StatefulSet** (souvent 3 rÃ©plicas) afin dâ€™assurer la cohÃ©rence et la haute disponibilitÃ©.

2. **Shards**  
   - Chaque shard contient une **partie des donnÃ©es** de la base.  
   - Chaque shard est lui-mÃªme un **ReplicaSet**, garantissant la redondance et la tolÃ©rance aux pannes.  
   - DÃ©ployÃ©s via :  
     ```yaml
     shard1-statefulset.yaml, shard2-statefulset.yaml
     ```
   - Chaque StatefulSet crÃ©e automatiquement des **PersistentVolumeClaims (PVC)** pour stocker durablement les donnÃ©es sur les disques associÃ©s.

3. **Mongos (Query Router)**  
   - Le composant **mongos** agit comme un **routeur de requÃªtes** : il reÃ§oit les requÃªtes clientes et les oriente vers le bon shard selon la clÃ© de sharding.  
   - Les clients et les applications interagissent **uniquement avec mongos**, jamais directement avec les shards.  
   - DÃ©ployÃ© via :  
     ```yaml
     mongos-deployment.yaml
     ```
   - Il peut Ãªtre rÃ©pliquÃ© (plusieurs pods mongos) pour supporter une charge importante et assurer la haute disponibilitÃ©.



#### Initialisation 

Pour se connecter au pod, on peut lancer cette commande : 
```
'kubectl exec -it mongos-xxxxxxxx -n test -- mongosh'
```

Puis executer les commandes suivantes afin d'initialiser le **config-server**, les **shards** et ensuite d'activer le **sharding** :

```
// Init config server
rs.initiate({
  _id: "rs-config-server",
  configsvr: true,
  members: [
    { _id: 0, host: "config-server-0.config-server.test.svc.cluster.local:27019" },
    { _id: 1, host: "config-server-1.config-server.test.svc.cluster.local:27019" },
    { _id: 2, host: "config-server-2.config-server.test.svc.cluster.local:27019" }
  ]
})

// Init shard1
rs.initiate({
  _id: "rs-shard1",
  members: [
    { _id: 0, host: "shard1-0.shard1.test.svc.cluster.local:27018" },
    { _id: 1, host: "shard1-1.shard1.test.svc.cluster.local:27018" },
    { _id: 2, host: "shard1-2.shard1.test.svc.cluster.local:27018" }
  ]
})

// Init shard2
rs.initiate({
  _id: "rs-shard2",
  members: [
    { _id: 0, host: "shard2-0.shard2.test.svc.cluster.local:27018" },
    { _id: 1, host: "shard2-1.shard2.test.svc.cluster.local:27018" },
    { _id: 2, host: "shard2-2.shard2.test.svc.cluster.local:27018" }
  ]
})

// Depuis mongos : ajouter les shards
sh.addShard("rs-shard1/shard1-0.shard1.test.svc.cluster.local:27018")
sh.addShard("rs-shard2/shard2-0.shard2.test.svc.cluster.local:27018")

// Activer le sharding sur la base
sh.enableSharding("shop")

// DÃ©finir la clÃ© de sharding
db.items.createIndex({ id: "hashed" })
sh.shardCollection("shop.items", { id: "hashed" })
```


### 5.4 Redis

Redis est utilisÃ© ici comme systÃ¨me de cache pour accÃ©lÃ©rer les requÃªtes en stockant temporairement les donnÃ©es. Il faut pour cela configurer un service pour accÃ©der aux pods et un statefulset pour gÃ©rer les pods avec un stockage persistant.

#### DÃ©ploiment

Reprendre les configs ` redis-service.yaml `et `redis-statefulset.yaml` et les apply avec : 
```
kubectl apply -f redis-service.yaml -n redis

kubectl apply -f redis-statefulset.yaml -n redis
```

VÃ©rifier que Redis est en cours dâ€™Ã©xecution: 

```
kubectl get pods -n redis -o wide

kubectl get pvc -n redis
```

#### Test

Afin de tester si les pods redis marchent bien il suffit de sâ€™y connecter avec : 

```
kubectl exec -it redis-0 -n redis -- redis-cli
```


Ensuite faire ces deux commandes pour obtenir la rÃ©ponse. : 

```
SET test "hello world"

GET test
```


#### IntÃ©gration

Pour intÃ©grer redis Ã  lâ€™app, il faut que dans le `deployment.yaml`, il y ait une value **redis.redis.svc.cluster.local** qui correspond au Service headless crÃ©e dans les configs. Ensuite dans votre app il faut l'importer avec : 
```
import redis
```


Et le setup : 
```
REDIS_HOST = os.getenv("REDIS_HOST", "redis.dev.svc.cluster.local")
REDIS_PORT = 6379 cache = redis.Redis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True) 
```



#### VÃ©rification

Pour notre app, Ã  chaque produit ajouter, on peut vÃ©rifier le cache simplement avec :
```
kubectl exec -it redis-0 -n dev -- redis-cli GET items
```



## 6. Guide pour les nouveaux developpeurs 
Ce guide a pour objectif dâ€™aider tout nouveau dÃ©veloppeur Ã  prendre en main le projet, du setup local jusquâ€™au dÃ©ploiement sur le cluster Kubernetes.

### 6.1 PrÃ©requis

Avant de commencer, assurez-vous dâ€™avoir installÃ© :

- **Python 3.10+**
- **Docker** (si vous souhaitez builder les images localement)
- **kubectl** (pour interagir avec le cluster Kubernetes)
- **Git** (pour cloner et contribuer au projet)
- **Un accÃ¨s au cluster Kubernetes** (namespace `dev` ou `test`)

### 6.2 Installation du projet

Cloner le dÃ©pÃ´t GitHub :
```
git clone https://github.com/avoineu/k8s.git

cd k8s
```

### 6.3 Lancer le cluster local (mode Kubernetes)

Appliquer les manifests Kubernetes pour dÃ©ployer les ressources nÃ©cessaires : 
```
kubectl apply -f k8s/
```

VÃ©rifier que les pods sont bien en cours dâ€™exÃ©cution : 
```
kubectl get pods -n dev
```

Pour visualiser les services et ingress :
```
kubectl get svc,ingress -n dev
```

### 6.4 Lancer lâ€™application 

Dans Kubernetes : `kubectl apply` suffit pour que les pods sâ€™exÃ©cutent automatiquement.

Si vous souhaitez simplement tester le code Python sans passer par Kubernetes :
```
python app/app.py
```
Lâ€™application Flask dÃ©marrera alors sur `http://127.0.0.1:5000`.

Ce mode est utile pour le dÃ©veloppement rapide ou le dÃ©bogage avant intÃ©gration au cluster.

### 6.5 Tests unitaires

Avant tout push, il est recommandÃ© dâ€™exÃ©cuter les tests unitaires :
```
pytest tests/
```
Aussi, un push sur la branche principale dÃ©clenchera automatiquement le pipeline GitHub Actions qui exÃ©cute ces tests dans le CI/CD.

### 6.6 DÃ©ploiement sur le cluster kubernetes

Pour (re)dÃ©ployer une version sur un namespace spÃ©cifique :
```
kubectl apply -f k8s/ -n dev
```
Pour un environnement de test :
```
kubectl apply -f k8s/ -n test
```
Ã€ noter que le namespace `prod` est rÃ©servÃ© aux versions stables et validÃ©es.

### 6.7 Logs & Debug
Afficher les logs dâ€™un pod en cours dâ€™exÃ©cution :
```
kubectl logs -f <nom-du-pod> -n dev
```
Entrer dans un pod pour le dÃ©bogage :
```
kubectl exec -it <nom-du-pod> -n dev -- /bin/bash
```
### 6.8 AccÃ¨s Ã  MongoDB pour inspection

Se connecter Ã  la base MongoDB :
```
kubectl exec -it mongodb-0 -n dev -- mongosh
```
Puis interroger la base :
```
use shop
db.items.find().pretty()
```

### 6.9 Bonnes pratiques 

* Tester vos changements localement **avant le push**.

* Ne jamais dÃ©ployer directement sur `prod` sans validation.

* Surveillez les pods avec :
```
kubectl get pods -A
```
* VÃ©rifiez rÃ©guliÃ¨rement lâ€™Ã©tat du pipeline GitHub Actions.